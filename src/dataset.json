{
    "TypeError": [
        {
            "message": "ie pinterest Fix extractor 12538 Closes 12529 Authored by mikf Co authored by UTF 8 q Mike 20F C3 A4hrmann mike_faehrmann web de",
            "changes": "@@ -23,9 +23,9 @@ class PinterestBaseIE(InfoExtractor):\n     def _call_api(self, resource, video_id, options):\n         return self._download_json(\n             f'https://www.pinterest.com/resource/{resource}Resource/get/',\n-            video_id, f'Download {resource} JSON metadata', query={\n-                'data': json.dumps({'options': options}),\n-            })['resource_response']\n+            video_id, f'Download {resource} JSON metadata',\n+            query={'data': json.dumps({'options': options})},\n+            headers={'X-Pinterest-PWS-Handler': 'www/[username].js'})['resource_response']\n \n     def _extract_video(self, data, extract_formats=True):\n         video_id = data['id']"
        },
        {
            "message": "ie twitter Fix syndication token generation 12537 Fix 14cd7f3443c6da4d49edaefcc12da9dee86e243e Authored by bashonly",
            "changes": "@@ -1334,7 +1334,7 @@ def _build_graphql_query(self, media_id):\n     def _generate_syndication_token(self, twid):\n         # ((Number(twid) / 1e15) * Math.PI).toString(36).replace(/(0+|\\.)/g, '')\n         translation = str.maketrans(dict.fromkeys('0.'))\n-        return js_number_to_string((int(twid) / 1e15) * math.PI, 36).translate(translation)\n+        return js_number_to_string((int(twid) / 1e15) * math.pi, 36).translate(translation)\n \n     def _call_syndication_api(self, twid):\n         self.report_warning("
        },
        {
            "message": "ie niconico live Fix thumbnail extraction 12419 Closes 12417 Authored by bashonly",
            "changes": "@@ -13,11 +13,13 @@\n     ExtractorError,\n     OnDemandPagedList,\n     clean_html,\n+    determine_ext,\n     float_or_none,\n     int_or_none,\n     join_nonempty,\n     parse_duration,\n     parse_iso8601,\n+    parse_qs,\n     parse_resolution,\n     qualities,\n     remove_start,\n@@ -1033,6 +1035,7 @@ def _real_extract(self, url):\n                 thumbnails.append({\n                     'id': f'{name}_{width}x{height}',\n                     'url': img_url,\n+                    'ext': traverse_obj(parse_qs(img_url), ('image', 0, {determine_ext(default_ext='jpg')})),\n                     **res,\n                 })\n "
        },
        {
            "message": "ie goplay Fix extractor 12237 Authored by alard",
            "changes": "@@ -12,7 +12,6 @@\n from ..utils import (\n     ExtractorError,\n     int_or_none,\n-    js_to_json,\n     remove_end,\n     traverse_obj,\n )\n@@ -76,6 +75,7 @@ def _real_initialize(self):\n         if not self._id_token:\n             raise self.raise_login_required(method='password')\n \n+    # XXX: For parsing next.js v15+ data; see also yt_dlp.extractor.francetv\n     def _find_json(self, s):\n         return self._search_json(\n             r'\\w+\\s*:\\s*', s, 'next js data', None, contains_pattern=r'\\[(?s:.+)\\]', default=None)\n@@ -86,9 +86,10 @@ def _real_extract(self, url):\n \n         nextjs_data = traverse_obj(\n             re.findall(r'<script[^>]*>\\s*self\\.__next_f\\.push\\(\\s*(\\[.+?\\])\\s*\\);?\\s*</script>', webpage),\n-            (..., {js_to_json}, {json.loads}, ..., {self._find_json}, ...))\n+            (..., {json.loads}, ..., {self._find_json}, ...))\n         meta = traverse_obj(nextjs_data, (\n-            ..., lambda _, v: v['meta']['path'] == urllib.parse.urlparse(url).path, 'meta', any))\n+            ..., ..., 'children', ..., ..., 'children',\n+            lambda _, v: v['video']['path'] == urllib.parse.urlparse(url).path, 'video', any))\n \n         video_id = meta['uuid']\n         info_dict = traverse_obj(meta, {"
        },
        {
            "message": "bark fix loading of generation config 36587",
            "changes": "@@ -1573,6 +1573,14 @@ def __init__(self, config):\n \n         self.config = config\n \n+    @classmethod\n+    def can_generate(cls) -> bool:\n+        # Bark has a unique model structure, where the external class (`BarkModel`) doesn't need to inherit from\n+        # `GenerationMixin` (it has a non-standard generation method), but one of the internal models do\n+        # (`BarkSemanticModel`). This means that the base `can_generate()` will return `False`, but we need to\n+        # override it so as to do `GenerationConfig` handling in multiple parts of the codebase.\n+        return True\n+\n     @property\n     def device(self) -> torch.device:\n         \"\"\""
        },
        {
            "message": "Fix _load_state_dict_into_meta_model with device_map None 36488 Fix _load_state_dict_into_meta_model with device_map None Update src transformers modeling_utils py",
            "changes": "@@ -785,8 +785,8 @@ def _load_state_dict_into_meta_model(\n     tensor_device = None\n     if device_map is not None and device_map.get(\"\", None) is not None:\n         tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n-\n-    device_map_regex = \"|\".join(sorted(device_map.keys(), reverse=True))\n+    if device_map is not None:\n+        device_map_regex = \"|\".join(sorted(device_map.keys(), reverse=True))\n \n     # we need this later to initialize tensor parallelism\n     if device_mesh is not None:"
        },
        {
            "message": "Fix hub_retry 36449 cry trigger Co authored by ydshieh ydshieh users noreply github com",
            "changes": "@@ -223,7 +223,7 @@ def __init_subclass__(cls, **kwargs):\n             if attr_name.startswith(\"test_\"):\n                 attr = getattr(cls, attr_name)\n                 if callable(attr):\n-                    setattr(cls, attr_name, hub_retry(attr))\n+                    setattr(cls, attr_name, hub_retry()(attr))\n \n     @property\n     def all_generative_model_classes(self):"
        },
        {
            "message": "Change raise NotImplemented to raise NotImplementedError 345",
            "changes": "@@ -66,7 +66,7 @@ def __init__(self, employee_id, name):\n         super(Operator, self).__init__(employee_id, name, Rank.DIRECTOR)\n \n     def escalate_call(self):\n-        raise NotImplemented('Directors must be able to handle any call')\n+        raise NotImplementedError('Directors must be able to handle any call')\n \n \n class CallState(Enum):"
        },
        {
            "message": "Fix invalid application of sizeof to an incomplete type 148854 Fixes with C 23 and constexpr std unique_ptr Pull Request resolved https github com pytorch pytorch pull 148854 Approved by https github com Skylion007",
            "changes": "@@ -446,9 +446,9 @@ class ForwardRef {{\n \n  public:\n   ForwardRef(): ptr_(std::make_unique<T>()) {{}}\n-  ForwardRef(ForwardRef<T>&&) = default;\n+  ForwardRef(ForwardRef<T>&&);\n   ForwardRef(const ForwardRef<T>& other): ptr_(std::make_unique<T>(*other.ptr_)) {{}}\n-  ForwardRef<T>& operator=(ForwardRef<T>&&) = default;\n+  ForwardRef<T>& operator=(ForwardRef<T>&&);\n   ForwardRef<T>& operator=(const ForwardRef<T>& other) {{\n     ptr_ = std::make_unique<T>(*other.ptr_);\n     return *this;\n@@ -521,6 +521,9 @@ class F64 {{\n {\"\".join(cpp_enum_defs.values())}\n {\"\".join(dict(sorted(cpp_class_defs.items(), key=lambda x: class_ordering[x[0]])).values())}\n {chr(10).join(cpp_json_defs)}\n+\n+template <typename T> ForwardRef<T>::ForwardRef(ForwardRef<T>&&) = default;\n+template <typename T> ForwardRef<T>& ForwardRef<T>::operator=(ForwardRef<T>&&) = default;\n }} // namespace _export\n }} // namespace torch\n \"\"\""
        },
        {
            "message": "Add ccode for FloorDiv 148727 Summary Add ccode for FloorDiv Test Plan CIs Differential Revision D70749021 Pull Request resolved https github com pytorch pytorch pull 148727 Approved by https github com bobrenjc93",
            "changes": "@@ -291,6 +291,11 @@ def eval(\n \n         return None\n \n+    def _ccode(self, printer):\n+        base = printer.parenthesize(self.base, PRECEDENCE[\"Atom\"] - 0.5)\n+        divisor = printer.parenthesize(self.divisor, PRECEDENCE[\"Atom\"] - 0.5)\n+        return f\"floor({base}/{divisor})\"\n+\n \n class ModularIndexing(sympy.Function):\n     \"\"\""
        },
        {
            "message": "JSInterp Handle undefined etc passed to JS_RegExp and Exception",
            "changes": "@@ -408,6 +408,7 @@ def __init__(self, code, objects=None):\n     class Exception(ExtractorError):\n         def __init__(self, msg, *args, **kwargs):\n             expr = kwargs.pop('expr', None)\n+            msg = str_or_none(msg, default='\"None\"')\n             if expr is not None:\n                 msg = '{0} in: {1!r:.100}'.format(msg.rstrip(), expr)\n             super(JSInterpreter.Exception, self).__init__(msg, *args, **kwargs)\n@@ -435,6 +436,7 @@ def __init__(self, pattern_txt, flags=0):\n                 flags, _ = self.regex_flags(flags)\n             # First, avoid https://github.com/python/cpython/issues/74534\n             self.__self = None\n+            pattern_txt = str_or_none(pattern_txt) or '(?:)'\n             self.__pattern_txt = pattern_txt.replace('[[', r'[\\[')\n             self.__flags = flags\n "
        },
        {
            "message": "compat Fix inheriting from compat_collections_chain_map see ytdl org youtube dl 33079 issuecomment 2704038049",
            "changes": "@@ -3473,11 +3473,12 @@ def pop(self, k, *args):\n         def new_child(self, m=None, **kwargs):\n             m = m or {}\n             m.update(kwargs)\n-            return compat_collections_chain_map(m, *self.maps)\n+            # support inheritance !\n+            return type(self)(m, *self.maps)\n \n         @property\n         def parents(self):\n-            return compat_collections_chain_map(*(self.maps[1:]))\n+            return type(self)(*(self.maps[1:]))\n \n \n # compat_re_Pattern, compat_re_Match"
        },
        {
            "message": "YouTube Fix 91b1569",
            "changes": "@@ -3305,7 +3305,7 @@ def _extract_lockup_view_model(self, view_model):\n             return\n         return merge_dicts(self.url_result(\n             update_url_query('https://www.youtube.com/playlist', {'list': content_id}),\n-            ie=YoutubeTabIE, video_id=content_id), {\n+            ie=YoutubeTabIE.ie_key(), video_id=content_id), {\n                 'title': traverse_obj(view_model, (\n                     'metadata', 'lockupMetadataViewModel', 'title', 'content', T(compat_str))),\n                 'thumbnails': self._extract_thumbnails(view_model, ("
        },
        {
            "message": "YouTube Endure subtitle URLs are complete WEB URLs are MWEB not resolves 33017",
            "changes": "@@ -2435,6 +2435,7 @@ def process_subtitles():\n                 subtitles = {}\n                 for caption_track in traverse_obj(pctr, (\n                         'captionTracks', lambda _, v: v.get('baseUrl'))):\n+                    base_url = self._yt_urljoin(caption_track['baseUrl'])\n                     if not base_url:\n                         continue\n                     if caption_track.get('kind') != 'asr':"
        },
        {
            "message": "fix backend Unbreak add_store_agent_to_library",
            "changes": "@@ -412,11 +412,8 @@ async def add_store_agent_to_library(\n             added_agent = await prisma.models.LibraryAgent.prisma().create(\n                 data={\n                     \"userId\": user_id,\n-                    \"Agent\": {\n-                        \"connect\": {\n-                            \"graphVersionId\": {\"id\": graph.id, \"version\": graph.version}\n-                        },\n-                    },\n+                    \"agentId\": graph.id,\n+                    \"agentVersion\": graph.version,\n                     \"isCreatedByUser\": False,\n                 },\n                 include=library_agent_include(user_id),"
        },
        {
            "message": "fix backend allow more than one arg on the tuple 9535 Clearly explain the need for these changes We allow tuples to be returned from exceptions but pydantic restricts their size to 1 b c the typehint This fixes that Changes Concisely describe all of the changes made in this pull request Adds to the tuple type hint Checklist For code changes x I have clearly listed my changes in the PR description x I have made a test plan x I have tested my changes according to the test plan Put your test plan here x Extracted from other pr where it was tested as part of an exception",
            "changes": "@@ -242,7 +242,7 @@ def cleanup(self):\n \n class RemoteCallError(BaseModel):\n     type: str = \"RemoteCallError\"\n-    args: Optional[Tuple[Any]] = None\n+    args: Optional[Tuple[Any, ...]] = None\n \n \n EXCEPTION_MAPPING = {"
        },
        {
            "message": "fix backend Use Pyro for RPC by default 9528 Follow up to 9508 HTTP based RPC has not been fully tested and should be disabled by default Changes Disable HTTP based RPC and use Pyro by default",
            "changes": "@@ -66,7 +66,7 @@ class Config(UpdateTrackingModel[\"Config\"], BaseSettings):\n         description=\"Maximum number of workers to use for node execution within a single graph.\",\n     )\n     use_http_based_rpc: bool = Field(\n-        default=True,\n+        default=False,\n         description=\"Whether to use HTTP-based RPC for communication between services.\",\n     )\n     pyro_host: str = Field("
        },
        {
            "message": "Corrected test case in ExclusionConstraintTests test_invalid_expressions",
            "changes": "@@ -309,7 +309,7 @@ def test_invalid_index_type(self):\n \n     def test_invalid_expressions(self):\n         msg = \"The expressions must be a list of 2-tuples.\"\n-        for expressions in ([\"foo\"], [\"foo\"], [(\"foo_1\", \"foo_2\", \"foo_3\")]):\n+        for expressions in ([\"foo\"], [(\"foo\",)], [(\"foo_1\", \"foo_2\", \"foo_3\")]):\n             with self.subTest(expressions), self.assertRaisesMessage(ValueError, msg):\n                 ExclusionConstraint(\n                     index_type=\"GIST\","
        }
    ],
    "KeyError": [
        {
            "message": "ie youtube Warn on missing formats due to SSAP 12483 See https github com yt dlp yt dlp issues 12482 Authored by coletdjnz",
            "changes": "@@ -4266,6 +4266,7 @@ def build_fragments(f):\n             } for range_start in range(0, f['filesize'], CHUNK_SIZE))\n \n         for fmt in streaming_formats:\n+            client_name = fmt[STREAMING_DATA_CLIENT_NAME]\n             if fmt.get('targetDurationSec'):\n                 continue\n \n@@ -4310,6 +4311,12 @@ def build_fragments(f):\n                 fmt_url = url_or_none(try_get(sc, lambda x: x['url'][0]))\n                 encrypted_sig = try_get(sc, lambda x: x['s'][0])\n                 if not all((sc, fmt_url, player_url, encrypted_sig)):\n+                    self.report_warning(\n+                        f'Some {client_name} client formats have been skipped as they are missing a url. '\n+                        f'{\"Your account\" if self.is_authenticated else \"The current session\"} may have '\n+                        f'the SSAP (server-side ads) experiment which may be interfering with yt-dlp. '\n+                        f'Please see  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details.',\n+                        only_once=True)\n                     continue\n                 try:\n                     fmt_url += '&{}={}'.format(\n@@ -4356,7 +4363,6 @@ def build_fragments(f):\n                 self.report_warning(\n                     f'{video_id}: Some formats are possibly damaged. They will be deprioritized', only_once=True)\n \n-            client_name = fmt[STREAMING_DATA_CLIENT_NAME]\n             po_token = fmt.get(STREAMING_DATA_INITIAL_PO_TOKEN)\n \n             if po_token:"
        },
        {
            "message": "Export base streamer 36500 Export base streamer Previously the base streamer class was not exported so the set of available streamers was fixed to 3 streamer classes This change makes it so that customers may extend the default base streamer class make fixup Co authored by Joao Gante joaofranciscocardosogante gmail com Co authored by Joao Gante joao huggingface co",
            "changes": "@@ -26,7 +26,7 @@\n         \"SynthIDTextWatermarkingConfig\",\n         \"WatermarkingConfig\",\n     ],\n-    \"streamers\": [\"AsyncTextIteratorStreamer\", \"TextIteratorStreamer\", \"TextStreamer\"],\n+    \"streamers\": [\"AsyncTextIteratorStreamer\", \"BaseStreamer\", \"TextIteratorStreamer\", \"TextStreamer\"],\n }\n \n try:\n@@ -197,7 +197,7 @@\n         SynthIDTextWatermarkingConfig,\n         WatermarkingConfig,\n     )\n-    from .streamers import AsyncTextIteratorStreamer, TextIteratorStreamer, TextStreamer\n+    from .streamers import AsyncTextIteratorStreamer, BaseStreamer, TextIteratorStreamer, TextStreamer\n \n     try:\n         if not is_torch_available():"
        },
        {
            "message": "Fix kwargs UserWarning in SamImageProcessor 36479 transformers image_processing_utils py 41 UserWarning The following named arguments are not valid for SamImageProcessor preprocess and were ignored point_pad_value",
            "changes": "@@ -106,6 +106,7 @@ def __call__(\n         input_points = output_kwargs[\"images_kwargs\"].pop(\"input_points\", None)\n         input_labels = output_kwargs[\"images_kwargs\"].pop(\"input_labels\", None)\n         input_boxes = output_kwargs[\"images_kwargs\"].pop(\"input_boxes\", None)\n+        point_pad_value = output_kwargs[\"images_kwargs\"].pop(\"point_pad_value\", None)\n \n         encoding_image_processor = self.image_processor(\n             images,\n@@ -131,7 +132,7 @@ def __call__(\n             input_labels=input_labels,\n             input_boxes=input_boxes,\n             return_tensors=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"),\n-            point_pad_value=output_kwargs[\"images_kwargs\"].get(\"point_pad_value\"),\n+            point_pad_value=point_pad_value,\n         )\n \n         return encoding_image_processor"
        },
        {
            "message": "Fix loading models with mismatched sizes 36463 Fix loading model with mismatched sizes trigger tests",
            "changes": "@@ -4907,7 +4907,9 @@ def _load_pretrained_model(\n                     model_to_load, state_dict, start_prefix\n                 )\n                 # at this point the state dict should be on cpu, we don't need to actually read it\n-                fixed_state_dict = model_to_load._fix_state_dict_keys_on_load(state_dict)\n+                mismatched_names = [name for name, _, _ in mismatched_keys]\n+                fixed_state_dict = {k: v for k, v in state_dict.items() if k not in mismatched_names}\n+                fixed_state_dict = model_to_load._fix_state_dict_keys_on_load(fixed_state_dict)\n                 model_to_load.load_state_dict(fixed_state_dict, strict=False, assign=assign_to_params_buffers)\n         else:\n             # This should always be a list but, just to be sure."
        },
        {
            "message": "Don t look at TESTING_ONLY in fuzzer 146870 Lots of configs aren t meant to be set because they re testing only Pull Request resolved https github com pytorch pytorch pull 146870 Approved by https github com masnesral",
            "changes": "@@ -811,6 +811,7 @@ def bisect(self, num_attempts: int = 100, p: float = 0.5) -> list[ConfigType]:\n                 if (\n                     field_name not in config\n                     and not field_name.startswith(\"_\")\n+                    and \"TESTING_ONLY\" not in field_name\n                     and random.random() < p\n                 ):\n                     value = self.sample("
        },
        {
            "message": "Update the comment 148726 Differential Revision D70747931 Pull Request resolved https github com pytorch pytorch pull 148726 Approved by https github com yf225",
            "changes": "@@ -303,7 +303,6 @@ def prologue_fusion_enabled() -> bool:\n mixed_mm_choice: Literal[\"default\", \"triton\", \"aten\", \"heuristic\"] = \"heuristic\"\n \n # enable reordering pass for increasing overlap between compute and communication\n-# only use with fsdp\n reorder_for_compute_comm_overlap = False\n \n # passes (in execution order) for increasing overlap between compute and communication"
        },
        {
            "message": "fix backend Append prompt into the conversations output Remove unused output pin on SmartDecisionBlock 9550 Changes Append prompt into the conversations output Remove unused output pin on SmartDecisionBlock Checklist For code changes I have clearly listed my changes in the PR description I have made a test plan I have tested my changes according to the test plan Put your test plan here details summary Example test plan summary Create from scratch and execute an agent with at least 3 blocks Import an agent from file upload and confirm it executes correctly Upload agent to marketplace Import an agent from marketplace and confirm it executes correctly Edit an agent from monitor and confirm it executes correctly details For configuration changes env example is updated or already compatible with my changes docker compose yml is updated or already compatible with my changes I have included a list of my configuration changes in the PR description under Changes details summary Examples of configuration changes summary Changing ports Adding new services that need to communicate with each other Secrets or environment variable changes New or infrastructure changes such as databases details",
            "changes": "@@ -79,12 +79,7 @@ class Input(BlockSchema):\n         )\n \n     class Output(BlockSchema):\n-\n-        prompt: str = SchemaField(description=\"The prompt sent to the language model.\")\n         error: str = SchemaField(description=\"Error message if the API call failed.\")\n-        function_signatures: list[dict[str, Any]] = SchemaField(\n-            description=\"The function signatures that are sent to the language model.\"\n-        )\n         tools: Any = SchemaField(description=\"The tools that are available to use.\")\n         finished: str = SchemaField(\n             description=\"The finished message to display to the user.\"\n@@ -331,6 +326,9 @@ def run(\n                 for c in response.tool_calls\n             )\n \n+        input_data.conversation_history.append(\n+            llm.Message(role=llm.MessageRole.USER, content=response.prompt)\n+        )\n         input_data.conversation_history.append(\n             llm.Message(role=llm.MessageRole.ASSISTANT, content=assistant_response)\n         )"
        },
        {
            "message": "fix platform Add Block Costs for SDM Block 9531 The Smart Decision Maker block needs to have costs associated with it Changes Added config for SDM Block Costs",
            "changes": "@@ -15,6 +15,7 @@\n     LlmModel,\n )\n from backend.blocks.replicate_flux_advanced import ReplicateFluxAdvancedModelBlock\n+from backend.blocks.smart_decision_maker import SmartDecisionMakerBlock\n from backend.blocks.talking_head import CreateTalkingAvatarVideoBlock\n from backend.blocks.text_to_speech_block import UnrealTextToSpeechBlock\n from backend.data.block import Block\n@@ -265,4 +266,5 @@\n             },\n         )\n     ],\n+    SmartDecisionMakerBlock: LLM_COST,\n }"
        },
        {
            "message": "Fix exception when an audio file with no speech is provided 1396 Co authored by Jong Wook Kim jongwook openai com",
            "changes": "@@ -145,7 +145,7 @@ def iterate_subtitles():\n             if len(subtitle) > 0:\n                 yield subtitle\n \n-        if \"words\" in result[\"segments\"][0]:\n+        if len(result[\"segments\"]) > 0 and \"words\" in result[\"segments\"][0]:\n             for subtitle in iterate_subtitles():\n                 subtitle_start = self.format_timestamp(subtitle[0][\"start\"])\n                 subtitle_end = self.format_timestamp(subtitle[-1][\"end\"])"
        }
    ],
    "ValueError": [
        {
            "message": "ie niconico Fix format sorting 12442 Authored by xpadev net",
            "changes": "@@ -28,6 +28,7 @@\n     try_get,\n     unescapeHTML,\n     update_url_query,\n+    url_basename,\n     url_or_none,\n     urlencode_postdata,\n     urljoin,\n@@ -432,6 +433,7 @@ def _yield_dms_formats(self, api_data, video_id):\n                     'format_id': ('id', {str}),\n                     'abr': ('bitRate', {float_or_none(scale=1000)}),\n                     'asr': ('samplingRate', {int_or_none}),\n+                    'quality': ('qualityLevel', {int_or_none}),\n                 }), get_all=False),\n                 'acodec': 'aac',\n             }\n@@ -443,7 +445,9 @@ def _yield_dms_formats(self, api_data, video_id):\n         min_abr = min(traverse_obj(audios, (..., 'bitRate', {float_or_none})), default=0) / 1000\n         for video_fmt in video_fmts:\n             video_fmt['tbr'] -= min_abr\n-            video_fmt['format_id'] = f'video-{video_fmt[\"tbr\"]:.0f}'\n+            video_fmt['format_id'] = url_basename(video_fmt['url']).rpartition('.')[0]\n+            video_fmt['quality'] = traverse_obj(videos, (\n+                lambda _, v: v['id'] == video_fmt['format_id'], 'qualityLevel', {int_or_none}, any)) or -1\n             yield video_fmt\n \n     def _real_extract(self, url):"
        },
        {
            "message": "ie lbry Raise appropriate error for non media files 12462 Closes 12182 Authored by bashonly",
            "changes": "@@ -26,6 +26,7 @@ class LBRYBaseIE(InfoExtractor):\n     _CLAIM_ID_REGEX = r'[0-9a-f]{1,40}'\n     _OPT_CLAIM_ID = f'[^$@:/?#&]+(?:[:#]{_CLAIM_ID_REGEX})?'\n     _SUPPORTED_STREAM_TYPES = ['video', 'audio']\n+    _UNSUPPORTED_STREAM_TYPES = ['binary']\n     _PAGE_SIZE = 50\n \n     def _call_api_proxy(self, method, display_id, params, resource):\n@@ -341,7 +342,7 @@ def _real_extract(self, url):\n                 HEADRequest(streaming_url), display_id, headers=headers,\n                 note='Downloading streaming redirect url info').url\n \n-        elif result.get('value_type') == 'stream':\n+        elif result.get('value_type') == 'stream' and stream_type not in self._UNSUPPORTED_STREAM_TYPES:\n             claim_id, is_live = result['signing_channel']['claim_id'], True\n             live_data = self._download_json(\n                 'https://api.odysee.live/livestream/is_live', claim_id,"
        },
        {
            "message": "ie instagram Fix extraction of older private posts 12451 Authored by bashonly",
            "changes": "@@ -33,8 +33,10 @@ def _pk_to_id(media_id):\n \n \n def _id_to_pk(shortcode):\n-    \"\"\"Covert a shortcode to a numeric value\"\"\"\n-    return decode_base_n(shortcode[:11], table=_ENCODING_CHARS)\n+    \"\"\"Convert a shortcode to a numeric value\"\"\"\n+    if len(shortcode) > 28:\n+        shortcode = shortcode[:-28]\n+    return decode_base_n(shortcode, table=_ENCODING_CHARS)\n \n \n class InstagramBaseIE(InfoExtractor):"
        },
        {
            "message": "Fix bamba tests amd 36535",
            "changes": "@@ -510,7 +510,7 @@ def test_simple_generate(self):\n         EXPECTED_TEXTS = {\n             # 7: \"\",\n             8: \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are all having a good time.\",\n-            #  9: \"\"\",\n+            9: \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n         }\n \n         self.model.to(torch_device)\n@@ -549,7 +549,10 @@ def test_simple_batched_generate_with_padding(self):\n                 \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n                 \"!!!<|begin_of_text|>I am late! I need to get to work! I have to get to the\",\n             ],\n-            9: [],\n+            9: [\n+                \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n+                \"!!!<|begin_of_text|>I am late! I need to be at the airport in 20 minutes! I\",\n+            ],\n         }\n \n         self.model.to(torch_device)"
        },
        {
            "message": "Check TRUST_REMOTE_CODE for RealmRetriever for security 36511 fix repush Co authored by ydshieh ydshieh users noreply github com",
            "changes": "@@ -21,7 +21,7 @@\n from huggingface_hub import hf_hub_download\n \n from .... import AutoTokenizer\n-from ....utils import logging\n+from ....utils import logging, strtobool\n \n \n _REALM_BLOCK_RECORDS_FILENAME = \"block_records.npy\"\n@@ -114,6 +114,14 @@ def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.P\n             block_records_path = hf_hub_download(\n                 repo_id=pretrained_model_name_or_path, filename=_REALM_BLOCK_RECORDS_FILENAME, **kwargs\n             )\n+        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+            raise ValueError(\n+                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is \"\n+                \"potentially malicious. It's recommended to never unpickle data that could have come from an \"\n+                \"untrusted source, or that could have been tampered with. If you already verified the pickle \"\n+                \"data and decided to use it, you can set the environment variable \"\n+                \"`TRUST_REMOTE_CODE` to `True` to allow it.\"\n+            )\n         block_records = np.load(block_records_path, allow_pickle=True)\n \n         tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, *init_inputs, **kwargs)"
        },
        {
            "message": "Fix Expected output for compressed tensors tests 36425 fix",
            "changes": "@@ -47,7 +47,7 @@ def test_config_to_from_dict(self):\n         self.assertIsInstance(config_from_dict.sparsity_config, SparsityCompressionConfig)\n \n     def test_tinyllama_w8a8(self):\n-        expected_out = \"<s> Paris is the capital of which country?\\n\\n**A) Paris**\\n\\n**Q** ** Paris is the capital of which country?\\n\\n**A) Paris**\\n\\n**Q** ** Paris is the capital of which country\"\n+        expected_out = \"<s> Paris is the capital of which country?\\n\\n  1. Paris is the capital of which country?\\n\\n  1. Paris is the capital of which country?\\n\\n  1. Paris is the capital of which country?\\n\\n\"\n         self._test_quantized_model(self.tinyllama_w8a8, expected_out)\n \n     def test_tinyllama_w4a16(self):\n@@ -59,7 +59,7 @@ def test_tinyllama_w8a16(self):\n         self._test_quantized_model(self.tinyllama_w8a16, expected_out)\n \n     def test_llama_8b_fp8(self):\n-        expected_out = \"<|begin_of_text|>Paris is the capital of which country? France\\nWhat is the name of the famous art museum in Paris? The Louvre\\nWhat is the name of the famous opera house in Paris? Palais Garnier\\nWhat is the name of the\"\n+        expected_out = \"<|begin_of_text|>Paris is the capital of which country? France\\nWhat is the name of the famous museum in Paris that is home to the Mona Lisa? The Louvre\\nWhat is the name of the famous bridge in Paris that is often associated with the city\"\n         self._test_quantized_model(self.llama3_8b_fp8, expected_out)\n \n     def _test_quantized_model(self, model_name: str, expected_output: str):"
        },
        {
            "message": "fix comment",
            "changes": "@@ -143,7 +143,7 @@ def linear(x: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] =\n         quantization-aware computations depending on the input parameters.\n \n     Notes:\n-        - If `weight` is quantized (e.g., `element_size() > 1`), a dequantized version \n+        - If `weight` is quantized (e.g., `element_size() == 1`), a dequantized version \n           is used for computation.\n         - If `gemm_impl == \"bf16\"`, dequantization and a `bf16` GEMM operation are applied.\n         - For other cases, the function applies quantization to `x` and uses `fp8_gemm` for computation."
        },
        {
            "message": "Fix Linear Layer Bias Initialization",
            "changes": "@@ -185,7 +185,7 @@ def __init__(self, in_features: int, out_features: int, bias: bool = False, dtyp\n         else:\n             self.register_parameter(\"scale\", None)\n         if bias:\n-            self.bias = nn.Parameter(torch.empty(self.part_out_features))\n+            self.bias = nn.Parameter(torch.empty(out_features))\n         else:\n             self.register_parameter(\"bias\", None)\n "
        },
        {
            "message": "Fix redistribution cost for all reduce 148761 This issue seems to have been introduced in https github com pytorch pytorch pull 119897 With the current implementation it might be more favorable to perform a reduce_scatter followed by an all gather than simply an all reduce Thanks lw for the helpful discussions on getting this PR out Pull Request resolved https github com pytorch pytorch pull 148761 Approved by https github com Skylion007 https github com lw https github com tianyu l https github com fegin",
            "changes": "@@ -297,7 +297,7 @@ def allreduce_cost(bytes_gb: float, mesh_topo: MeshTopoInfo, mesh_dim: int) -> f\n     num_devices_on_mesh_dim = mesh_topo.mesh_dim_devices[mesh_dim]\n     mesh_dim_bandwidth = mesh_topo.mesh_dim_bandwidth[mesh_dim]\n     # allreduce have almost 2x comm bytes compare to allgather/reduce_scatter\n-    num_hops = 2 * num_devices_on_mesh_dim - 1\n+    num_hops = 2 * (num_devices_on_mesh_dim - 1)\n \n     latency = 6.6 + num_hops * mesh_topo.mesh_dim_latency[mesh_dim]\n     bw = (bytes_gb * num_hops / num_devices_on_mesh_dim) / mesh_dim_bandwidth"
        },
        {
            "message": "fix backend Filter out empty object from conversation_history pin on SmartDecisionMakerBlock",
            "changes": "@@ -338,7 +338,7 @@ def run(\n         tool_functions = self._create_function_signature(node_id)\n \n         input_data.conversation_history = input_data.conversation_history or []\n-        prompt = [json.to_dict(p) for p in input_data.conversation_history]\n+        prompt = [json.to_dict(p) for p in input_data.conversation_history if p]\n \n         pending_tool_calls = get_pending_tool_calls(input_data.conversation_history)\n         if pending_tool_calls and not input_data.last_tool_output:"
        },
        {
            "message": "Fix not available source in Onkyo 140175",
            "changes": "@@ -588,7 +588,7 @@ def process_update(self, update: tuple[str, str, Any]) -> None:\n             self._attr_volume_level = min(1, volume_level)\n         elif command in [\"muting\", \"audio-muting\"]:\n             self._attr_is_volume_muted = bool(value == \"on\")\n-        elif command in [\"selector\", \"input-selector\"]:\n+        elif command in [\"selector\", \"input-selector\"] and value != \"N/A\":\n             self._parse_source(value)\n             self._query_av_info_delayed()\n         elif command == \"hdmi-output-selector\":"
        },
        {
            "message": "Fix split function to handle trailing delimiters correctly 12423 Fix split function to handle trailing delimiters correctly pre commit ci auto fixes from pre commit com hooks for more information see https pre commit ci Update split py Co authored by pre commit ci bot 66853113 pre commit ci bot users noreply github com Co authored by Maxim Smolskiy mithridatus mail ru",
            "changes": "@@ -14,6 +14,9 @@ def split(string: str, separator: str = \" \") -> list:\n \n     >>> split(\"12:43:39\",separator = \":\")\n     ['12', '43', '39']\n+\n+    >>> split(\";abbb;;c;\", separator=';')\n+    ['', 'abbb', '', 'c', '']\n     \"\"\"\n \n     split_words = []\n@@ -23,7 +26,7 @@ def split(string: str, separator: str = \" \") -> list:\n         if char == separator:\n             split_words.append(string[last_index:index])\n             last_index = index + 1\n-        elif index + 1 == len(string):\n+        if index + 1 == len(string):\n             split_words.append(string[last_index : index + 1])\n     return split_words\n "
        }
    ],
    "IndexError": [
        {
            "message": "ie francetv site Fix livestream extraction 12316 Closes 12310 Authored by bashonly",
            "changes": "@@ -358,7 +358,8 @@ def _real_extract(self, url):\n             # For livestreams we need the id of the stream instead of the currently airing episode id\n             video_id = traverse_obj(nextjs_data, (\n                 ..., ..., 'children', ..., 'children', ..., 'children', ..., 'children', ..., ...,\n-                'children', ..., ..., 'children', ..., ..., 'children', ..., 'options', 'id', {str}, any))\n+                'children', ..., ..., 'children', ..., ..., 'children', (..., (..., ...)),\n+                'options', 'id', {str}, any))\n         else:\n             video_id = traverse_obj(nextjs_data, (\n                 ..., ..., ..., 'children',"
        },
        {
            "message": "avoid errors when the size of input_ids passed to PrefixConstrainedLogitsProcessor is zero 36489 avoid errors when the size of input_ids passed to PrefixConstrainedLogitsProcessor is zero use more reasonable process avoid early return Co authored by Joao Gante joaofranciscocardosogante gmail com",
            "changes": "@@ -1353,8 +1353,11 @@ def __init__(self, prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         mask = torch.full_like(scores, -math.inf)\n-        for batch_id, beam_sent in enumerate(input_ids.view(-1, self._num_beams, input_ids.shape[-1])):\n-            for beam_id, sent in enumerate(beam_sent):\n+        batch_size = input_ids.shape[0] // self._num_beams\n+\n+        for batch_id in range(batch_size):\n+            for beam_id in range(self._num_beams):\n+                sent = input_ids[batch_id * self._num_beams + beam_id]\n                 prefix_allowed_tokens = self._prefix_allowed_tokens_fn(batch_id, sent)\n                 if len(prefix_allowed_tokens) == 0:\n                     raise ValueError("
        },
        {
            "message": "fix scores mask",
            "changes": "@@ -585,8 +585,8 @@ def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n             else:\n                 group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n             indices = group_scores.topk(self.topk_groups, dim=-1)[1]\n-            mask = torch.zeros_like(scores[..., 0]).scatter_(1, indices, True)\n-            scores = (scores * mask.unsqueeze(-1)).flatten(1)\n+            mask = scores.new_ones(x.size(0), self.n_groups, dtype=bool).scatter_(1, indices, False)\n+            scores = scores.masked_fill_(mask.unsqueeze(-1), float(\"-inf\")).flatten(1)\n         indices = torch.topk(scores, self.topk, dim=-1)[1]\n         weights = original_scores.gather(1, indices)\n         if self.score_func == \"sigmoid\":"
        },
        {
            "message": "JSInterp Make indexing error handling more conformant by default TypeError undefined else raise set allow_undefined True False to override",
            "changes": "@@ -672,14 +672,15 @@ def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion)\n         except Exception as e:\n             raise self.Exception('Failed to evaluate {left_val!r:.50} {op} {right_val!r:.50}'.format(**locals()), expr, cause=e)\n \n-    def _index(self, obj, idx, allow_undefined=True):\n+    def _index(self, obj, idx, allow_undefined=None):\n         if idx == 'length' and isinstance(obj, list):\n             return len(obj)\n         try:\n             return obj[int(idx)] if isinstance(obj, list) else obj[compat_str(idx)]\n         except (TypeError, KeyError, IndexError) as e:\n-            if allow_undefined:\n-                # when is not allowed?\n+            # allow_undefined is None gives correct behaviour\n+            if allow_undefined or (\n+                    allow_undefined is None and not isinstance(e, TypeError)):\n                 return JS_Undefined\n             raise self.Exception('Cannot get index {idx!r:.100}'.format(**locals()), expr=repr(obj), cause=e)\n "
        },
        {
            "message": "jsinterp Further improve expression parsing fix fd8242e Passes tests from yt dlp",
            "changes": "@@ -804,16 +804,19 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             if op in ('+', '-'):\n                 # simplify/adjust consecutive instances of these operators\n                 undone = 0\n-                while len(separated) > 1 and not separated[-1].strip():\n+                separated = [s.strip() for s in separated]\n+                while len(separated) > 1 and not separated[-1]:\n                     undone += 1\n                     separated.pop()\n                 if op == '-' and undone % 2 != 0:\n                     right_expr = op + right_expr\n                 elif op == '+':\n-                    while len(separated) > 1 and separated[-1].strip() in self.OP_CHARS:\n+                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                    if separated[-1][-1:] in self.OP_CHARS:\n                         right_expr = separated.pop() + right_expr\n                 # hanging op at end of left => unary + (strip) or - (push right)\n-                left_val = separated[-1]\n+                left_val = separated[-1] if separated else ''\n                 for dm_op in ('*', '%', '/', '**'):\n                     bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n                     if len(bodmas) > 1 and not bodmas[-1].strip():"
        },
        {
            "message": "fix bugs in lora support",
            "changes": "@@ -398,7 +398,7 @@ def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Li\n     if weights_backup is not None:\r\n         if isinstance(self, torch.nn.MultiheadAttention):\r\n             restore_weights_backup(self, 'in_proj_weight', weights_backup[0])\r\n-            restore_weights_backup(self.out_proj, 'weight', weights_backup[0])\r\n+            restore_weights_backup(self.out_proj, 'weight', weights_backup[1])\r\n         else:\r\n             restore_weights_backup(self, 'weight', weights_backup)\r\n \r\n@@ -437,7 +437,7 @@ def network_apply_weights(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn\n     bias_backup = getattr(self, \"network_bias_backup\", None)\r\n     if bias_backup is None and wanted_names != ():\r\n         if isinstance(self, torch.nn.MultiheadAttention) and self.out_proj.bias is not None:\r\n-            bias_backup = store_weights_backup(self.out_proj)\r\n+            bias_backup = store_weights_backup(self.out_proj.bias)\r\n         elif getattr(self, 'bias', None) is not None:\r\n             bias_backup = store_weights_backup(self.bias)\r\n         else:\r"
        }
    ],
    "VersionError": [
        {
            "message": "guard torch version for uint16 36520 u16 style fix",
            "changes": "@@ -522,17 +522,19 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n     \"U8\": torch.uint8,\n     \"I8\": torch.int8,\n     \"I16\": torch.int16,\n-    \"U16\": torch.uint16,\n     \"F16\": torch.float16,\n     \"BF16\": torch.bfloat16,\n     \"I32\": torch.int32,\n-    \"U32\": torch.uint32,\n     \"F32\": torch.float32,\n     \"F64\": torch.float64,\n     \"I64\": torch.int64,\n-    \"U64\": torch.uint64,\n }\n \n+if is_torch_greater_or_equal(\"2.3.0\"):\n+    str_to_torch_dtype[\"U16\"] = torch.uint16\n+    str_to_torch_dtype[\"U32\"] = torch.uint32\n+    str_to_torch_dtype[\"U64\"] = torch.uint64\n+\n \n def load_state_dict(\n     checkpoint_file: Union[str, os.PathLike],"
        },
        {
            "message": "YouTube Update TVHTML5 client parameters resolves 33078",
            "changes": "@@ -122,7 +122,8 @@ class YoutubeBaseInfoExtractor(InfoExtractor):\n             'INNERTUBE_CONTEXT': {\n                 'client': {\n                     'clientName': 'TVHTML5',\n-                    'clientVersion': '7.20241201.18.00',\n+                    'clientVersion': '7.20250120.19.00',\n+                    'userAgent': 'Mozilla/5.0 (ChromiumStylePlatform) Cobalt/Version',\n                 },\n             },\n             'INNERTUBE_CONTEXT_CLIENT_NAME': 7,"
        },
        {
            "message": "Fix version not always available in onewire 140260",
            "changes": "@@ -2,6 +2,7 @@\n \n from __future__ import annotations\n \n+import contextlib\n from datetime import datetime, timedelta\n import logging\n import os\n@@ -58,7 +59,7 @@ class OneWireHub:\n \n     owproxy: protocol._Proxy\n     devices: list[OWDeviceDescription]\n-    _version: str\n+    _version: str | None = None\n \n     def __init__(self, hass: HomeAssistant, config_entry: OneWireConfigEntry) -> None:\n         \"\"\"Initialize.\"\"\"\n@@ -74,7 +75,9 @@ def _initialize(self) -> None:\n         port = self._config_entry.data[CONF_PORT]\n         _LOGGER.debug(\"Initializing connection to %s:%s\", host, port)\n         self.owproxy = protocol.proxy(host, port)\n-        self._version = self.owproxy.read(protocol.PTH_VERSION).decode()\n+        with contextlib.suppress(protocol.OwnetError):\n+            # Version is not available on all servers\n+            self._version = self.owproxy.read(protocol.PTH_VERSION).decode()\n         self.devices = _discover_devices(self.owproxy)\n \n     async def initialize(self) -> None:"
        }
    ],
    "EnvironmentError": [
        {
            "message": "Skip distributed subprocess test internally as they don t work 148909 Follow up from https github com pytorch pytorch pull 146098 Pull Request resolved https github com pytorch pytorch pull 148909 Approved by https github com janeyx99",
            "changes": "@@ -8,6 +8,7 @@\n import tempfile\n import threading\n import time\n+import unittest\n from contextlib import nullcontext\n from dataclasses import dataclass\n from datetime import timedelta\n@@ -35,6 +36,8 @@\n )\n from torch.testing._internal.common_utils import (\n     instantiate_parametrized_tests,\n+    IS_FBCODE,\n+    IS_SANDCASTLE,\n     load_tests,\n     parametrize,\n     retry_on_connect_failures,\n@@ -1908,6 +1911,7 @@ def test_init_process_group_for_all_backends(self):\n \n             dist.destroy_process_group()\n \n+    @unittest.skipIf(IS_FBCODE or IS_SANDCASTLE, \"subprocess test fails in fbcode\")\n     def test_default_process_group(self):\n         script = \"\"\"\n # Hide all GPUs"
        },
        {
            "message": "Inductor Windows add env_var switch to turn all Windows inductor UTs 148733 For timeout reason we can t turn on all Windows Inductor UTs in CI https github com pytorch pytorch issues 135927 And without the UTs we can t ensure Windows inductor quality Intel team will do some local test for Windows inductor but we still need to add a switch to turn on the full Windows inductor UTs The switch is an environment variable cmd set TORCHINDUCTOR_WINDOWS_TESTS 1 After setup this environment variable we can turn on all Windows inductor UTs It will not affect to PyTorch CI Pull Request resolved https github com pytorch pytorch pull 148733 Approved by https github com jansel Co authored by Jason Ansel jansel jansel net",
            "changes": "@@ -11,6 +11,7 @@\n import contextlib\n import importlib\n import logging\n+import os\n from typing import Union\n \n import torch\n@@ -35,7 +36,11 @@ def run_tests(needs: Union[str, tuple[str, ...]] = ()) -> None:\n     if TEST_WITH_TORCHDYNAMO or TEST_WITH_CROSSREF:\n         return  # skip testing\n \n-    if not torch.xpu.is_available() and IS_WINDOWS:\n+    if (\n+        not torch.xpu.is_available()\n+        and IS_WINDOWS\n+        and os.environ.get(\"TORCHINDUCTOR_WINDOWS_TESTS\", \"0\") == \"0\"\n+    ):\n         return\n \n     if isinstance(needs, str):"
        }
    ],
    "Flaky": [
        {
            "message": "Add timm_efficientnet to flaky models after cuda 12 6 update in CI CD 148788 After https github com pytorch pytorch pull 148612 This model have become flaky Tracking this regression in an issue https github com pytorch pytorch issues 148699 Pull Request resolved https github com pytorch pytorch pull 148788 Approved by https github com izaitsevfb https github com malfet",
            "changes": "@@ -12,6 +12,7 @@\n     \"yolov3\",\n     \"gluon_inception_v3\",\n     \"detectron2_maskrcnn_r_101_c4\",\n+    \"timm_efficientnet\",  # see https://github.com/pytorch/pytorch/issues/148699\n     \"XGLMForCausalLM\",  # discovered in https://github.com/pytorch/pytorch/pull/128148\n }\n "
        },
        {
            "message": "fix backend Increase logging level threshold of RPC service to WARNING 9576 Increase the logging threshold to WARNING to avoid chatty RPC service request logs Before RPC calls show up in backend log https github com user attachments assets 70791f87 12ef 4a12 8343 4b8641302faa",
            "changes": "@@ -326,7 +326,12 @@ def __start_fastapi(self):\n             f\"[{self.service_name}] Starting RPC server at http://{api_host}:{self.get_port()}\"\n         )\n         server = uvicorn.Server(\n-            uvicorn.Config(self.fastapi_app, host=api_host, port=self.get_port())\n+            uvicorn.Config(\n+                self.fastapi_app,\n+                host=api_host,\n+                port=self.get_port(),\n+                log_level=\"warning\",\n+            )\n         )\n         self.shared_event_loop.run_until_complete(server.serve())\n "
        },
        {
            "message": "handling transcribe exceptions 1682 handling transcribe exceptions printing stacktrace Co authored by invalid invalid email com Co authored by Jong Wook Kim jongwook nyu edu Co authored by Jong Wook Kim jongwook openai com",
            "changes": "@@ -1,5 +1,6 @@\n import argparse\n import os\n+import traceback\n import warnings\n from typing import TYPE_CHECKING, Optional, Tuple, Union\n \n@@ -468,8 +469,12 @@ def valid_model_name(name):\n         warnings.warn(\"--max_words_per_line has no effect with --max_line_width\")\n     writer_args = {arg: args.pop(arg) for arg in word_options}\n     for audio_path in args.pop(\"audio\"):\n-        result = transcribe(model, audio_path, temperature=temperature, **args)\n-        writer(result, audio_path, **writer_args)\n+        try:\n+            result = transcribe(model, audio_path, temperature=temperature, **args)\n+            writer(result, audio_path, **writer_args)\n+        except Exception as e:\n+            traceback.print_exc()\n+            print(f\"Skipping {audio_path} due to {type(e).__name__}: {str(e)}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "message": "fix lint",
            "changes": "@@ -119,7 +119,7 @@ def get_sigmas(self, p, steps):\n \r\n             if scheduler.need_inner_model:\r\n                 sigmas_kwargs['inner_model'] = self.model_wrap\r\n-            \r\n+\r\n             if scheduler.label == 'Beta':\r\n                 p.extra_generation_params[\"Beta schedule alpha\"] = opts.beta_dist_alpha\r\n                 p.extra_generation_params[\"Beta schedule beta\"] = opts.beta_dist_beta\r"
        },
        {
            "message": "1248 Skip a failing test when running on Windows",
            "changes": "@@ -1,5 +1,7 @@\n # -*- encoding: utf-8 -*-\n \n+import pytest\n+import sys\n from mock import Mock, patch\n from psutil import AccessDenied, TimeoutExpired\n \n@@ -30,6 +32,7 @@ def test_get_output_invalid_continuation_byte(self, popen_mock):\n         actual = rerun.get_output('', '')\n         assert actual == expected\n \n+    @pytest.mark.skipif(sys.platform == 'win32', reason=\"skip when running on Windows\")\n     @patch('thefuck.output_readers.rerun._wait_output')\n     def test_get_output_unicode_misspell(self, wait_output_mock):\n         rerun.get_output(u'p\u00e1cman', u'p\u00e1cman')"
        }
    ],
    "NullPointer": [
        {
            "message": "YouTube Avoid early crash if webpage can t be read see issue 33013",
            "changes": "@@ -1951,7 +1951,7 @@ def _real_extract(self, url):\n             pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n \n             player_url = self._extract_player_url(webpage)\n-            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage or '')\n             sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n             if sts:\n                 pb_context['signatureTimestamp'] = sts"
        },
        {
            "message": "Fix events without user in Bring integration 140213 Fix events without publicUserUuid",
            "changes": "@@ -77,9 +77,12 @@ def _async_handle_event(self) -> None:\n             attributes = asdict(activity.content)\n \n             attributes[\"last_activity_by\"] = next(\n-                x.name\n-                for x in bring_list.users.users\n-                if x.publicUuid == activity.content.publicUserUuid\n+                (\n+                    x.name\n+                    for x in bring_list.users.users\n+                    if x.publicUuid == activity.content.publicUserUuid\n+                ),\n+                None,\n             )\n \n             self._trigger_event("
        },
        {
            "message": "Make checkpoint_states an instance variable of CheckpointManager 2273 As per https github com 3b1b manim issues 2272",
            "changes": "@@ -157,7 +157,8 @@ def checkpoint_paste(\n \n \n class CheckpointManager:\n-    checkpoint_states: dict[str, list[tuple[Mobject, Mobject]]] = dict()\n+    def __init__(self):\n+        self.checkpoint_states: dict[str, list[tuple[Mobject, Mobject]]] = dict()\n \n     def checkpoint_paste(self, shell, scene):\n         \"\"\""
        }
    ],
    "AttributeError": [
        {
            "message": "utils Save orig_msg in ExtractorError",
            "changes": "@@ -2406,7 +2406,7 @@ def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None):\n         \"\"\" tb, if given, is the original traceback (so that it can be printed out).\n         If expected is set, this is a normal error message and most likely not a bug in youtube-dl.\n         \"\"\"\n-\n+        self.orig_msg = msg\n         if sys.exc_info()[0] in (compat_urllib_error.URLError, socket.timeout, UnavailableVideoError):\n             expected = True\n         if video_id is not None:"
        },
        {
            "message": "Fix build 12516 Empty commit Fix pre commit ci auto fixes from pre commit com hooks for more information see https pre commit ci Fix Apply suggestions from code review Co authored by pre commit ci bot 66853113 pre commit ci bot users noreply github com Co authored by Christian Clauss cclauss me com",
            "changes": "@@ -15,7 +15,7 @@\n def stock_price(symbol: str = \"AAPL\") -> str:\n     \"\"\"\n     >>> stock_price(\"EEEE\")\n-    '-'\n+    '- '\n     >>> isinstance(float(stock_price(\"GOOG\")),float)\n     True\n     \"\"\"\n@@ -24,12 +24,10 @@ def stock_price(symbol: str = \"AAPL\") -> str:\n         url, headers={\"USER-AGENT\": \"Mozilla/5.0\"}, timeout=10\n     ).text\n     soup = BeautifulSoup(yahoo_finance_source, \"html.parser\")\n-    specific_fin_streamer_tag = soup.find(\"fin-streamer\", {\"data-testid\": \"qsp-price\"})\n \n-    if specific_fin_streamer_tag:\n-        text = specific_fin_streamer_tag.get_text()\n-        return text\n-    return \"No <fin-streamer> tag with the specified data-test attribute found.\"\n+    if specific_fin_streamer_tag := soup.find(\"span\", {\"data-testid\": \"qsp-price\"}):\n+        return specific_fin_streamer_tag.get_text()\n+    return \"No <fin-streamer> tag with the specified data-testid attribute found.\"\n \n \n # Search for the symbol at https://finance.yahoo.com/lookup"
        }
    ],
    "LogicError": [
        {
            "message": "Bugfix Illogical Avoid computing higher temperatures on no_speech 1903 Bugfix Illogical Avoid computing higher temperatures on no_speech Bugfix for https github com openai whisper pull 1279 It s silence when decoding has failed due to compression_ratio_threshold too when further down the code it s not silence anymore Silence should be only when decoding has failed due to logprob_threshold Like described there https github com openai whisper blob 8bc8860694949db53c42ba47ddc23786c2e02a8b whisper transcribe py L421 And in code there https github com openai whisper blob 8bc8860694949db53c42ba47ddc23786c2e02a8b whisper transcribe py L243 L251 Fix if logprob_threshold None Co authored by Jong Wook Kim jongwook openai com",
            "changes": "@@ -214,6 +214,8 @@ def decode_with_fallback(segment: torch.Tensor) -> DecodingResult:\n             if (\n                 no_speech_threshold is not None\n                 and decode_result.no_speech_prob > no_speech_threshold\n+                and logprob_threshold is not None\n+                and decode_result.avg_logprob < logprob_threshold\n             ):\n                 needs_fallback = False  # silence\n             if not needs_fallback:"
        },
        {
            "message": "fix condition_on_previous_text 1224 prompt_reset_since is set before all_tokens is extended hence does not have the expected effect",
            "changes": "@@ -312,10 +312,6 @@ def new_segment(\n                 )\n                 seek += segment_size\n \n-            if not condition_on_previous_text or result.temperature > 0.5:\n-                # do not feed the prompt tokens if a high temperature was used\n-                prompt_reset_since = len(all_tokens)\n-\n             if word_timestamps:\n                 add_word_timestamps(\n                     segments=current_segments,\n@@ -361,6 +357,10 @@ def new_segment(\n                 [token for segment in current_segments for token in segment[\"tokens\"]]\n             )\n \n+            if not condition_on_previous_text or result.temperature > 0.5:\n+                # do not feed the prompt tokens if a high temperature was used\n+                prompt_reset_since = len(all_tokens)\n+\n             # update progress bar\n             pbar.update(min(content_frames, seek) - previous_seek)\n "
        },
        {
            "message": "Update decoding py 1155 Update decoding py Following the suggestions of Jeronymous in https github com openai whisper pull 914 and https github com openai whisper discussions 924 it solves the problem of endless loop Removed blank line and whitespaces in empty lines Suggested changes according to the linter Co authored by Jong Wook Kim jongwook openai com",
            "changes": "@@ -471,6 +471,13 @@ def apply(self, logits: Tensor, tokens: Tensor):\n                 # timestamps shouldn't decrease; forbid timestamp tokens smaller than the last\n                 logits[k, self.tokenizer.timestamp_begin : timestamps[-1]] = -np.inf\n \n+                # to force that timestamps are strictly increasing\n+                if last_was_timestamp and not penultimate_was_timestamp:\n+                    timestamp_last = timestamps[-1]\n+                else:\n+                    timestamp_last = timestamps[-1] + 1\n+                logits[k, self.tokenizer.timestamp_begin : timestamp_last] = -np.inf\n+\n         if tokens.shape[1] == self.sample_begin:\n             # suppress generating non-timestamp tokens at the beginning\n             logits[:, : self.tokenizer.timestamp_begin] = -np.inf"
        },
        {
            "message": "Ensure that HTTPDigest only raises an exception when auto_error is True 2939 Co authored by svlandeg sofie vanlandeghem gmail com",
            "changes": "@@ -37,8 +37,8 @@ def test_security_http_digest_incorrect_scheme_credentials():\n     response = client.get(\n         \"/users/me\", headers={\"Authorization\": \"Other invalidauthorization\"}\n     )\n-    assert response.status_code == 403, response.text\n-    assert response.json() == {\"detail\": \"Invalid authentication credentials\"}\n+    assert response.status_code == 200, response.text\n+    assert response.json() == {\"msg\": \"Create an account first\"}\n \n \n def test_openapi_schema():"
        },
        {
            "message": "Log broad exception in Electricity Maps config flow 140219",
            "changes": "@@ -3,11 +3,11 @@\n from __future__ import annotations\n \n from collections.abc import Mapping\n+import logging\n from typing import Any\n \n from aioelectricitymaps import (\n     ElectricityMaps,\n-    ElectricityMapsError,\n     ElectricityMapsInvalidTokenError,\n     ElectricityMapsNoDataError,\n )\n@@ -36,6 +36,8 @@\n TYPE_SPECIFY_COORDINATES = \"specify_coordinates\"\n TYPE_SPECIFY_COUNTRY = \"specify_country_code\"\n \n+_LOGGER = logging.getLogger(__name__)\n+\n \n class ElectricityMapsConfigFlow(ConfigFlow, domain=DOMAIN):\n     \"\"\"Handle a config flow for Co2signal.\"\"\"\n@@ -158,7 +160,8 @@ async def _validate_and_create(\n                 errors[\"base\"] = \"invalid_auth\"\n             except ElectricityMapsNoDataError:\n                 errors[\"base\"] = \"no_data\"\n-            except ElectricityMapsError:\n+            except Exception:\n+                _LOGGER.exception(\"Unexpected error occurred while checking API key\")\n                 errors[\"base\"] = \"unknown\"\n             else:\n                 if self.source == SOURCE_REAUTH:"
        },
        {
            "message": "In TensorFlow Object Detection fixed the order of the arguments when calling KeypointEstimationParams super class s __new__ method Specifically the order for the last two elements gaussian_denom_ratio argmax_postprocessing are corrected PiperOrigin RevId 733826226",
            "changes": "@@ -2384,7 +2384,7 @@ def __new__(cls,\n         offset_head_num_filters, offset_head_kernel_sizes,\n         regress_head_num_filters, regress_head_kernel_sizes,\n         score_distance_multiplier, std_dev_multiplier, rescoring_threshold,\n-        argmax_postprocessing, gaussian_denom_ratio)\n+        gaussian_denom_ratio, argmax_postprocessing)\n \n \n class ObjectCenterParams("
        },
        {
            "message": "Update beta sampling code in augment py The function _sample_from_beta alpha beta shape in MixupAndCutmix class is not having the same functionality as numpy random beta So tfm vision augment MixupAndCutmix _sample_from_beta 0 2 0 2 tf shape tf range 10000 numpy is also deviating as well So suggesting the fix keeping alpha alpha beta 1 0 in _sample_from_beta The reproduced gist https colab sandbox google com gist LakshmiKalaKadali 06533824610d6e85ea4aa3c6399819e6 tf_model_13490 ipynb scrollTo zSlE 3YDjL91 also attached This PR closes 13490 https github com tensorflow models issues 13490 Thank You",
            "changes": "@@ -2697,8 +2697,8 @@ def distort(self, images: tf.Tensor,\n \n   @staticmethod\n   def _sample_from_beta(alpha, beta, shape):\n-    sample_alpha = tf.random.gamma(shape, 1., beta=alpha)\n-    sample_beta = tf.random.gamma(shape, 1., beta=beta)\n+    sample_alpha = tf.random.gamma(shape, alpha, beta=1.0)\n+    sample_beta = tf.random.gamma(shape, alpha, beta=1.0)\n     return sample_alpha / (sample_alpha + sample_beta)\n \n   def _cutmix(self, images: tf.Tensor,"
        },
        {
            "message": "fix upscale logic",
            "changes": "@@ -56,8 +56,8 @@ def upscale(self, img: PIL.Image, scale, selected_model: str = None):\n         dest_w = int((img.width * scale) // 8 * 8)\n         dest_h = int((img.height * scale) // 8 * 8)\n \n-        for _ in range(3):\n-            if img.width >= dest_w and img.height >= dest_h and scale != 1:\n+        for i in range(3):\n+            if img.width >= dest_w and img.height >= dest_h and (i > 0 or scale != 1):\n                 break\n \n             if shared.state.interrupted:"
        },
        {
            "message": "Fix error in avl_tree del_node function 11510 fixed error in del_node function Update avl_tree py Co authored by Maxim Smolskiy mithridatus mail ru",
            "changes": "@@ -221,6 +221,10 @@ def del_node(root: MyNode, data: Any) -> MyNode | None:\n     else:\n         root.set_right(del_node(right_child, data))\n \n+    # Re-fetch left_child and right_child references\n+    left_child = root.get_left()\n+    right_child = root.get_right()\n+\n     if get_height(right_child) - get_height(left_child) == 2:\n         assert right_child is not None\n         if get_height(right_child.get_right()) > get_height(right_child.get_left()):"
        },
        {
            "message": "Doomsday Algorithm Fix leap year check 12396 Fix leap year check Replace in year 400 0 line 49 with Justification Years that are divisible by 100 centurian 100 but not by 400 year 400 0 are skipped and NOT leap year Update parentheses Correct the parentheses to make clear the precedence of the conditional check Update other doomsday py Co authored by Tianyi Zheng tianyizheng02 gmail com Co authored by Tianyi Zheng tianyizheng02 gmail com",
            "changes": "@@ -46,7 +46,7 @@ def get_week_day(year: int, month: int, day: int) -> str:\n     ) % 7\n     day_anchor = (\n         DOOMSDAY_NOT_LEAP[month - 1]\n-        if (year % 4 != 0) or (centurian == 0 and (year % 400) == 0)\n+        if year % 4 != 0 or (centurian == 0 and year % 400 != 0)\n         else DOOMSDAY_LEAP[month - 1]\n     )\n     week_day = (dooms_day + day - day_anchor) % 7"
        },
        {
            "message": "Fix dynamic_programming longest_increasing_subsequence py 12517 Fix 12510 Added the doctest mentioned in the issue pre commit ci auto fixes from pre commit com hooks for more information see https pre commit ci Fixed Grammer Mistake Update longest_increasing_subsequence py Update longest_increasing_subsequence py Update longest_increasing_subsequence py Update longest_increasing_subsequence py Co authored by pre commit ci bot 66853113 pre commit ci bot users noreply github com Co authored by Maxim Smolskiy mithridatus mail ru",
            "changes": "@@ -24,8 +24,10 @@ def longest_subsequence(array: list[int]) -> list[int]:  # This function is recu\n     [10, 22, 33, 41, 60, 80]\n     >>> longest_subsequence([4, 8, 7, 5, 1, 12, 2, 3, 9])\n     [1, 2, 3, 9]\n+    >>> longest_subsequence([28, 26, 12, 23, 35, 39])\n+    [12, 23, 35, 39]\n     >>> longest_subsequence([9, 8, 7, 6, 5, 7])\n-    [8]\n+    [5, 7]\n     >>> longest_subsequence([1, 1, 1])\n     [1, 1, 1]\n     >>> longest_subsequence([])\n@@ -44,7 +46,7 @@ def longest_subsequence(array: list[int]) -> list[int]:  # This function is recu\n     while not is_found and i < array_length:\n         if array[i] < pivot:\n             is_found = True\n-            temp_array = [element for element in array[i:] if element >= array[i]]\n+            temp_array = array[i:]\n             temp_array = longest_subsequence(temp_array)\n             if len(temp_array) > len(longest_subseq):\n                 longest_subseq = temp_array"
        }
    ],
    "Documentation": [
        {
            "message": "fix doc of TextDecoder 1526 Signed off by haoshengqiang haoshengqiang xiaohongshu com Co authored by haoshengqiang haoshengqiang xiaohongshu com",
            "changes": "@@ -197,7 +197,7 @@ def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n         \"\"\"\n         x : torch.LongTensor, shape = (batch_size, <= n_ctx)\n             the text tokens\n-        xa : torch.Tensor, shape = (batch_size, n_mels, n_audio_ctx)\n+        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)\n             the encoded audio features to be attended on\n         \"\"\"\n         offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0"
        },
        {
            "message": "Add Doc test bubble sort 12070 The string manipulation replace pre commit ci auto fixes from pre commit com hooks for more information see https pre commit ci Update replace py pre commit ci auto fixes from pre commit com hooks for more information see https pre commit ci updating DIRECTORY md Add doc test to bubble_sort Update DIRECTORY md Delete strings replace py Update bubble_sort py Co authored by pre commit ci bot 66853113 pre commit ci bot users noreply github com Co authored by vijayalaxmi777 vijayalaxmi777 users noreply github com Co authored by Maxim Smolskiy mithridatus mail ru",
            "changes": "@@ -85,6 +85,8 @@ def bubble_sort_recursive(collection: list[Any]) -> list[Any]:\n     [1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7]\n     >>> bubble_sort_recursive([1, 3.3, 5, 7.7, 2, 4.4, 6])\n     [1, 2, 3.3, 4.4, 5, 6, 7.7]\n+    >>> bubble_sort_recursive(['a', 'Z', 'B', 'C', 'A', 'c'])\n+    ['A', 'B', 'C', 'Z', 'a', 'c']\n     >>> import random\n     >>> collection_arg = random.sample(range(-50, 50), 100)\n     >>> bubble_sort_recursive(collection_arg) == sorted(collection_arg)"
        }
    ],
    "SyntaxError": [
        {
            "message": "fix 16169 Py 3 9 compatibility Co Authored By SLAPaper Pang slapaper pku gmail com",
            "changes": "@@ -118,7 +118,7 @@ def apply_size(p, x: str, xs) -> None:\n \r\n \r\n def find_vae(name: str):\r\n-    if name := name.strip().lower() in ('auto', 'automatic'):\r\n+    if (name := name.strip().lower()) in ('auto', 'automatic'):\r\n         return 'Automatic'\r\n     elif name == 'none':\r\n         return 'None'\r"
        },
        {
            "message": "fix Add missing comma Add missing comma in a test",
            "changes": "@@ -13,7 +13,7 @@ def output():\n     [\n         \"$ cd newdir\",\n         \" $ cd newdir\",\n-        \"$ $ cd newdir\"\n+        \"$ $ cd newdir\",\n         \" $ $ cd newdir\",\n     ],\n )"
        }
    ]
}