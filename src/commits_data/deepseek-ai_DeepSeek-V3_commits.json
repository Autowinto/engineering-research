{
  "repo_name": "deepseek-ai/DeepSeek-V3",
  "commits": [
    {
      "sha": "1398800ebfcd49c048737e8b1aae69dee46ffefc",
      "message": "fix scores mask",
      "changes": [
        {
          "file": "inference/model.py",
          "patch": "@@ -585,8 +585,8 @@ def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n             else:\n                 group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n             indices = group_scores.topk(self.topk_groups, dim=-1)[1]\n-            mask = torch.zeros_like(scores[..., 0]).scatter_(1, indices, True)\n-            scores = (scores * mask.unsqueeze(-1)).flatten(1)\n+            mask = scores.new_ones(x.size(0), self.n_groups, dtype=bool).scatter_(1, indices, False)\n+            scores = scores.masked_fill_(mask.unsqueeze(-1), float(\"-inf\")).flatten(1)\n         indices = torch.topk(scores, self.topk, dim=-1)[1]\n         weights = original_scores.gather(1, indices)\n         if self.score_func == \"sigmoid\":"
        }
      ]
    },
    {
      "sha": "5ee97a83f0457d0d805b862aeb387358e1801e6d",
      "message": "fix comment",
      "changes": [
        {
          "file": "inference/model.py",
          "patch": "@@ -143,7 +143,7 @@ def linear(x: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] =\n         quantization-aware computations depending on the input parameters.\n \n     Notes:\n-        - If `weight` is quantized (e.g., `element_size() > 1`), a dequantized version \n+        - If `weight` is quantized (e.g., `element_size() == 1`), a dequantized version \n           is used for computation.\n         - If `gemm_impl == \"bf16\"`, dequantization and a `bf16` GEMM operation are applied.\n         - For other cases, the function applies quantization to `x` and uses `fp8_gemm` for computation."
        }
      ]
    },
    {
      "sha": "6a30b43249a5710a3adb18c11763222d3fca8756",
      "message": "Fix Linear Layer Bias Initialization",
      "changes": [
        {
          "file": "inference/model.py",
          "patch": "@@ -185,7 +185,7 @@ def __init__(self, in_features: int, out_features: int, bias: bool = False, dtyp\n         else:\n             self.register_parameter(\"scale\", None)\n         if bias:\n-            self.bias = nn.Parameter(torch.empty(self.part_out_features))\n+            self.bias = nn.Parameter(torch.empty(out_features))\n         else:\n             self.register_parameter(\"bias\", None)\n "
        }
      ]
    }
  ]
}