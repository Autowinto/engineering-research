{
  "repo_name": "openai/whisper",
  "commits": [
    {
      "sha": "90db0de1896c23cbfaf0c58bc2d30665f709f170",
      "message": "Bugfix: Illogical \"Avoid computing higher temperatures on no_speech\" (#1903)\n\n* Bugfix: Illogical \"Avoid computing higher temperatures on no_speech\"\r\n\r\nBugfix for https://github.com/openai/whisper/pull/1279\r\n\r\nIt's \"silence\" when decoding has failed due to `compression_ratio_threshold` too, when further down the code it's not \"silence\" anymore.\r\n\r\n\"Silence\" should be only when decoding has failed due to `logprob_threshold`.\r\n\r\nLike described there:\r\nhttps://github.com/openai/whisper/blob/8bc8860694949db53c42ba47ddc23786c2e02a8b/whisper/transcribe.py#L421\r\n\r\nAnd in code there:\r\nhttps://github.com/openai/whisper/blob/8bc8860694949db53c42ba47ddc23786c2e02a8b/whisper/transcribe.py#L243-L251\r\n\r\n* Fix if \"logprob_threshold=None\"\r\n\r\n---------\r\n\r\nCo-authored-by: Jong Wook Kim <jongwook@openai.com>",
      "changes": [
        {
          "file": "whisper/transcribe.py",
          "patch": "@@ -214,6 +214,8 @@ def decode_with_fallback(segment: torch.Tensor) -> DecodingResult:\n             if (\n                 no_speech_threshold is not None\n                 and decode_result.no_speech_prob > no_speech_threshold\n+                and logprob_threshold is not None\n+                and decode_result.avg_logprob < logprob_threshold\n             ):\n                 needs_fallback = False  # silence\n             if not needs_fallback:"
        }
      ]
    },
    {
      "sha": "b7d277acd59c19edab3c75b8bf362ddd27fddcc7",
      "message": "handling transcribe exceptions. (#1682)\n\n* handling transcribe() exceptions.\r\n\r\n* printing stacktrace\r\n\r\n---------\r\n\r\nCo-authored-by: invalid <invalid@email.com>\r\nCo-authored-by: Jong Wook Kim <jongwook@nyu.edu>\r\nCo-authored-by: Jong Wook Kim <jongwook@openai.com>",
      "changes": [
        {
          "file": "whisper/transcribe.py",
          "patch": "@@ -1,5 +1,6 @@\n import argparse\n import os\n+import traceback\n import warnings\n from typing import TYPE_CHECKING, Optional, Tuple, Union\n \n@@ -468,8 +469,12 @@ def valid_model_name(name):\n         warnings.warn(\"--max_words_per_line has no effect with --max_line_width\")\n     writer_args = {arg: args.pop(arg) for arg in word_options}\n     for audio_path in args.pop(\"audio\"):\n-        result = transcribe(model, audio_path, temperature=temperature, **args)\n-        writer(result, audio_path, **writer_args)\n+        try:\n+            result = transcribe(model, audio_path, temperature=temperature, **args)\n+            writer(result, audio_path, **writer_args)\n+        except Exception as e:\n+            traceback.print_exc()\n+            print(f\"Skipping {audio_path} due to {type(e).__name__}: {str(e)}\")\n \n \n if __name__ == \"__main__\":"
        }
      ]
    },
    {
      "sha": "b38a1f20f4b23f3f3099af2c3e0ca95627276ddf",
      "message": "Fix exception when an audio file with no speech is provided (#1396)\n\nCo-authored-by: Jong Wook Kim <jongwook@openai.com>",
      "changes": [
        {
          "file": "whisper/utils.py",
          "patch": "@@ -145,7 +145,7 @@ def iterate_subtitles():\n             if len(subtitle) > 0:\n                 yield subtitle\n \n-        if \"words\" in result[\"segments\"][0]:\n+        if len(result[\"segments\"]) > 0 and \"words\" in result[\"segments\"][0]:\n             for subtitle in iterate_subtitles():\n                 subtitle_start = self.format_timestamp(subtitle[0][\"start\"])\n                 subtitle_end = self.format_timestamp(subtitle[-1][\"end\"])"
        }
      ]
    },
    {
      "sha": "21010ef454fb25954b0914785180311fb077add9",
      "message": "fix doc of TextDecoder (#1526)\n\nSigned-off-by: haoshengqiang <haoshengqiang@xiaohongshu.com>\r\nCo-authored-by: haoshengqiang <haoshengqiang@xiaohongshu.com>",
      "changes": [
        {
          "file": "whisper/model.py",
          "patch": "@@ -197,7 +197,7 @@ def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n         \"\"\"\n         x : torch.LongTensor, shape = (batch_size, <= n_ctx)\n             the text tokens\n-        xa : torch.Tensor, shape = (batch_size, n_mels, n_audio_ctx)\n+        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)\n             the encoded audio features to be attended on\n         \"\"\"\n         offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0"
        }
      ]
    },
    {
      "sha": "248b6cb124225dd263bb9bd32d060b6517e067f8",
      "message": "fix condition_on_previous_text (#1224)\n\nprompt_reset_since is set before all_tokens is extended hence does not have the expected effect.",
      "changes": [
        {
          "file": "whisper/transcribe.py",
          "patch": "@@ -312,10 +312,6 @@ def new_segment(\n                 )\n                 seek += segment_size\n \n-            if not condition_on_previous_text or result.temperature > 0.5:\n-                # do not feed the prompt tokens if a high temperature was used\n-                prompt_reset_since = len(all_tokens)\n-\n             if word_timestamps:\n                 add_word_timestamps(\n                     segments=current_segments,\n@@ -361,6 +357,10 @@ def new_segment(\n                 [token for segment in current_segments for token in segment[\"tokens\"]]\n             )\n \n+            if not condition_on_previous_text or result.temperature > 0.5:\n+                # do not feed the prompt tokens if a high temperature was used\n+                prompt_reset_since = len(all_tokens)\n+\n             # update progress bar\n             pbar.update(min(content_frames, seek) - previous_seek)\n "
        }
      ]
    },
    {
      "sha": "b0022b3283232b2b9f19262360cd80ec9975aeb4",
      "message": "Update decoding.py (#1155)\n\n* Update decoding.py\r\n\r\nFollowing the suggestions of @Jeronymous in https://github.com/openai/whisper/pull/914 and https://github.com/openai/whisper/discussions/924, it solves the problem of endless loop.\r\n\r\n* Removed blank line and whitespaces in empty lines.\r\n\r\n* Suggested changes according to the linter\r\n\r\n---------\r\n\r\nCo-authored-by: Jong Wook Kim <jongwook@openai.com>",
      "changes": [
        {
          "file": "whisper/decoding.py",
          "patch": "@@ -471,6 +471,13 @@ def apply(self, logits: Tensor, tokens: Tensor):\n                 # timestamps shouldn't decrease; forbid timestamp tokens smaller than the last\n                 logits[k, self.tokenizer.timestamp_begin : timestamps[-1]] = -np.inf\n \n+                # to force that timestamps are strictly increasing\n+                if last_was_timestamp and not penultimate_was_timestamp:\n+                    timestamp_last = timestamps[-1]\n+                else:\n+                    timestamp_last = timestamps[-1] + 1\n+                logits[k, self.tokenizer.timestamp_begin : timestamp_last] = -np.inf\n+\n         if tokens.shape[1] == self.sample_begin:\n             # suppress generating non-timestamp tokens at the beginning\n             logits[:, : self.tokenizer.timestamp_begin] = -np.inf"
        }
      ]
    }
  ]
}