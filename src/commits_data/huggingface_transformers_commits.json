{
  "repo_name": "huggingface/transformers",
  "commits": [
    {
      "sha": "8a16edce671e28a20f5469be66c7e097fb4fce33",
      "message": "Export base streamer. (#36500)\n\n* Export base streamer. \n\nPreviously, the base streamer class was not exported so the set of available streamers was fixed to 3 streamer classes. \r\n\r\nThis change makes it so that customers may extend the default base streamer class.\n\n* make fixup\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\nCo-authored-by: Joao Gante <joao@huggingface.co>",
      "changes": [
        {
          "file": "src/transformers/generation/__init__.py",
          "patch": "@@ -26,7 +26,7 @@\n         \"SynthIDTextWatermarkingConfig\",\n         \"WatermarkingConfig\",\n     ],\n-    \"streamers\": [\"AsyncTextIteratorStreamer\", \"TextIteratorStreamer\", \"TextStreamer\"],\n+    \"streamers\": [\"AsyncTextIteratorStreamer\", \"BaseStreamer\", \"TextIteratorStreamer\", \"TextStreamer\"],\n }\n \n try:\n@@ -197,7 +197,7 @@\n         SynthIDTextWatermarkingConfig,\n         WatermarkingConfig,\n     )\n-    from .streamers import AsyncTextIteratorStreamer, TextIteratorStreamer, TextStreamer\n+    from .streamers import AsyncTextIteratorStreamer, BaseStreamer, TextIteratorStreamer, TextStreamer\n \n     try:\n         if not is_torch_available():"
        }
      ]
    },
    {
      "sha": "6f775970c7713d04d238a2e63a0a0ea4d5e87ba7",
      "message": "avoid errors when the size of `input_ids` passed to `PrefixConstrainedLogitsProcessor` is zero (#36489)\n\n* avoid errors when the size of `input_ids` passed to PrefixConstrainedLogitsProcessor is zero\n\n* use more reasonable process\n\n* avoid early return\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
      "changes": [
        {
          "file": "src/transformers/generation/logits_process.py",
          "patch": "@@ -1353,8 +1353,11 @@ def __init__(self, prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         mask = torch.full_like(scores, -math.inf)\n-        for batch_id, beam_sent in enumerate(input_ids.view(-1, self._num_beams, input_ids.shape[-1])):\n-            for beam_id, sent in enumerate(beam_sent):\n+        batch_size = input_ids.shape[0] // self._num_beams\n+\n+        for batch_id in range(batch_size):\n+            for beam_id in range(self._num_beams):\n+                sent = input_ids[batch_id * self._num_beams + beam_id]\n                 prefix_allowed_tokens = self._prefix_allowed_tokens_fn(batch_id, sent)\n                 if len(prefix_allowed_tokens) == 0:\n                     raise ValueError("
        }
      ]
    },
    {
      "sha": "c1b24c0b73f11b14f5ad2c77f68ea8b7fa214cfb",
      "message": "[bark] fix loading of generation config (#36587)",
      "changes": [
        {
          "file": "src/transformers/models/bark/modeling_bark.py",
          "patch": "@@ -1573,6 +1573,14 @@ def __init__(self, config):\n \n         self.config = config\n \n+    @classmethod\n+    def can_generate(cls) -> bool:\n+        # Bark has a unique model structure, where the external class (`BarkModel`) doesn't need to inherit from\n+        # `GenerationMixin` (it has a non-standard generation method), but one of the internal models do\n+        # (`BarkSemanticModel`). This means that the base `can_generate()` will return `False`, but we need to\n+        # override it so as to do `GenerationConfig` handling in multiple parts of the codebase.\n+        return True\n+\n     @property\n     def device(self) -> torch.device:\n         \"\"\""
        }
      ]
    },
    {
      "sha": "752ef3fd4e70869626ec70657a770a85c0ad9219",
      "message": "guard torch version for uint16 (#36520)\n\n* u16\n\n* style\n\n* fix",
      "changes": [
        {
          "file": "src/transformers/modeling_utils.py",
          "patch": "@@ -522,17 +522,19 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n     \"U8\": torch.uint8,\n     \"I8\": torch.int8,\n     \"I16\": torch.int16,\n-    \"U16\": torch.uint16,\n     \"F16\": torch.float16,\n     \"BF16\": torch.bfloat16,\n     \"I32\": torch.int32,\n-    \"U32\": torch.uint32,\n     \"F32\": torch.float32,\n     \"F64\": torch.float64,\n     \"I64\": torch.int64,\n-    \"U64\": torch.uint64,\n }\n \n+if is_torch_greater_or_equal(\"2.3.0\"):\n+    str_to_torch_dtype[\"U16\"] = torch.uint16\n+    str_to_torch_dtype[\"U32\"] = torch.uint32\n+    str_to_torch_dtype[\"U64\"] = torch.uint64\n+\n \n def load_state_dict(\n     checkpoint_file: Union[str, os.PathLike],"
        }
      ]
    },
    {
      "sha": "c0c5acff077ac7c8fe68a0fdbad24306dbd9d4e3",
      "message": "Fix bamba tests amd (#36535)",
      "changes": [
        {
          "file": "tests/models/bamba/test_modeling_bamba.py",
          "patch": "@@ -510,7 +510,7 @@ def test_simple_generate(self):\n         EXPECTED_TEXTS = {\n             # 7: \"\",\n             8: \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are all having a good time.\",\n-            #  9: \"\"\",\n+            9: \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n         }\n \n         self.model.to(torch_device)\n@@ -549,7 +549,10 @@ def test_simple_batched_generate_with_padding(self):\n                 \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n                 \"!!!<|begin_of_text|>I am late! I need to get to work! I have to get to the\",\n             ],\n-            9: [],\n+            9: [\n+                \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n+                \"!!!<|begin_of_text|>I am late! I need to be at the airport in 20 minutes! I\",\n+            ],\n         }\n \n         self.model.to(torch_device)"
        }
      ]
    },
    {
      "sha": "3e83ee75ec7a5291c95a7172d90665eed450bfba",
      "message": "Fix kwargs UserWarning in SamImageProcessor (#36479)\n\ntransformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `SamImageProcessor.preprocess` and were ignored: 'point_pad_value'",
      "changes": [
        {
          "file": "src/transformers/models/sam/processing_sam.py",
          "patch": "@@ -106,6 +106,7 @@ def __call__(\n         input_points = output_kwargs[\"images_kwargs\"].pop(\"input_points\", None)\n         input_labels = output_kwargs[\"images_kwargs\"].pop(\"input_labels\", None)\n         input_boxes = output_kwargs[\"images_kwargs\"].pop(\"input_boxes\", None)\n+        point_pad_value = output_kwargs[\"images_kwargs\"].pop(\"point_pad_value\", None)\n \n         encoding_image_processor = self.image_processor(\n             images,\n@@ -131,7 +132,7 @@ def __call__(\n             input_labels=input_labels,\n             input_boxes=input_boxes,\n             return_tensors=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"),\n-            point_pad_value=output_kwargs[\"images_kwargs\"].get(\"point_pad_value\"),\n+            point_pad_value=point_pad_value,\n         )\n \n         return encoding_image_processor"
        }
      ]
    },
    {
      "sha": "9e3a1072c25732d351a09218d8e8843148ecc3c9",
      "message": "Check `TRUST_REMOTE_CODE` for `RealmRetriever` for security (#36511)\n\n* fix\n\n* repush\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
      "changes": [
        {
          "file": "src/transformers/models/deprecated/realm/retrieval_realm.py",
          "patch": "@@ -21,7 +21,7 @@\n from huggingface_hub import hf_hub_download\n \n from .... import AutoTokenizer\n-from ....utils import logging\n+from ....utils import logging, strtobool\n \n \n _REALM_BLOCK_RECORDS_FILENAME = \"block_records.npy\"\n@@ -114,6 +114,14 @@ def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.P\n             block_records_path = hf_hub_download(\n                 repo_id=pretrained_model_name_or_path, filename=_REALM_BLOCK_RECORDS_FILENAME, **kwargs\n             )\n+        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+            raise ValueError(\n+                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is \"\n+                \"potentially malicious. It's recommended to never unpickle data that could have come from an \"\n+                \"untrusted source, or that could have been tampered with. If you already verified the pickle \"\n+                \"data and decided to use it, you can set the environment variable \"\n+                \"`TRUST_REMOTE_CODE` to `True` to allow it.\"\n+            )\n         block_records = np.load(block_records_path, allow_pickle=True)\n \n         tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, *init_inputs, **kwargs)"
        }
      ]
    },
    {
      "sha": "dcbdf7e962c4b36140cc9ee76f870016121e69e5",
      "message": "Fix _load_state_dict_into_meta_model with device_map=None (#36488)\n\n* Fix _load_state_dict_into_meta_model with device_map=None\n\n* Update src/transformers/modeling_utils.py",
      "changes": [
        {
          "file": "src/transformers/modeling_utils.py",
          "patch": "@@ -785,8 +785,8 @@ def _load_state_dict_into_meta_model(\n     tensor_device = None\n     if device_map is not None and device_map.get(\"\", None) is not None:\n         tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n-\n-    device_map_regex = \"|\".join(sorted(device_map.keys(), reverse=True))\n+    if device_map is not None:\n+        device_map_regex = \"|\".join(sorted(device_map.keys(), reverse=True))\n \n     # we need this later to initialize tensor parallelism\n     if device_mesh is not None:"
        }
      ]
    },
    {
      "sha": "02776d2c6aa997c5b81f28f2edf38df9967253be",
      "message": "Fix loading models with mismatched sizes (#36463)\n\n* Fix loading model with mismatched sizes\n\n* trigger tests",
      "changes": [
        {
          "file": "src/transformers/modeling_utils.py",
          "patch": "@@ -4907,7 +4907,9 @@ def _load_pretrained_model(\n                     model_to_load, state_dict, start_prefix\n                 )\n                 # at this point the state dict should be on cpu, we don't need to actually read it\n-                fixed_state_dict = model_to_load._fix_state_dict_keys_on_load(state_dict)\n+                mismatched_names = [name for name, _, _ in mismatched_keys]\n+                fixed_state_dict = {k: v for k, v in state_dict.items() if k not in mismatched_names}\n+                fixed_state_dict = model_to_load._fix_state_dict_keys_on_load(fixed_state_dict)\n                 model_to_load.load_state_dict(fixed_state_dict, strict=False, assign=assign_to_params_buffers)\n         else:\n             # This should always be a list but, just to be sure."
        }
      ]
    },
    {
      "sha": "482d17be60d1836a7e787c5c3d69d4c3ed171ebc",
      "message": "Fix `hub_retry` (#36449)\n\n* cry\n\n* trigger\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
      "changes": [
        {
          "file": "tests/test_modeling_common.py",
          "patch": "@@ -223,7 +223,7 @@ def __init_subclass__(cls, **kwargs):\n             if attr_name.startswith(\"test_\"):\n                 attr = getattr(cls, attr_name)\n                 if callable(attr):\n-                    setattr(cls, attr_name, hub_retry(attr))\n+                    setattr(cls, attr_name, hub_retry()(attr))\n \n     @property\n     def all_generative_model_classes(self):"
        }
      ]
    },
    {
      "sha": "a7fbab33aeb865010f752e3840304f4b2cddb31b",
      "message": "Fix Expected output for compressed-tensors tests (#36425)\n\nfix",
      "changes": [
        {
          "file": "tests/quantization/compressed_tensors/test_compressed_tensors.py",
          "patch": "@@ -47,7 +47,7 @@ def test_config_to_from_dict(self):\n         self.assertIsInstance(config_from_dict.sparsity_config, SparsityCompressionConfig)\n \n     def test_tinyllama_w8a8(self):\n-        expected_out = \"<s> Paris is the capital of which country?\\n\\n**A) Paris**\\n\\n**Q** ** Paris is the capital of which country?\\n\\n**A) Paris**\\n\\n**Q** ** Paris is the capital of which country\"\n+        expected_out = \"<s> Paris is the capital of which country?\\n\\n  1. Paris is the capital of which country?\\n\\n  1. Paris is the capital of which country?\\n\\n  1. Paris is the capital of which country?\\n\\n\"\n         self._test_quantized_model(self.tinyllama_w8a8, expected_out)\n \n     def test_tinyllama_w4a16(self):\n@@ -59,7 +59,7 @@ def test_tinyllama_w8a16(self):\n         self._test_quantized_model(self.tinyllama_w8a16, expected_out)\n \n     def test_llama_8b_fp8(self):\n-        expected_out = \"<|begin_of_text|>Paris is the capital of which country? France\\nWhat is the name of the famous art museum in Paris? The Louvre\\nWhat is the name of the famous opera house in Paris? Palais Garnier\\nWhat is the name of the\"\n+        expected_out = \"<|begin_of_text|>Paris is the capital of which country? France\\nWhat is the name of the famous museum in Paris that is home to the Mona Lisa? The Louvre\\nWhat is the name of the famous bridge in Paris that is often associated with the city\"\n         self._test_quantized_model(self.llama3_8b_fp8, expected_out)\n \n     def _test_quantized_model(self, model_name: str, expected_output: str):"
        }
      ]
    }
  ]
}