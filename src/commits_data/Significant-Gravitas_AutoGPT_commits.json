{
  "repo_name": "Significant-Gravitas/AutoGPT",
  "commits": [
    {
      "sha": "9210d448eea78c80e1668e8e2f814b5632c29d82",
      "message": "fix(backend): Unbreak `add_store_agent_to_library`",
      "changes": [
        {
          "file": "autogpt_platform/backend/backend/server/v2/library/db.py",
          "patch": "@@ -412,11 +412,8 @@ async def add_store_agent_to_library(\n             added_agent = await prisma.models.LibraryAgent.prisma().create(\n                 data={\n                     \"userId\": user_id,\n-                    \"Agent\": {\n-                        \"connect\": {\n-                            \"graphVersionId\": {\"id\": graph.id, \"version\": graph.version}\n-                        },\n-                    },\n+                    \"agentId\": graph.id,\n+                    \"agentVersion\": graph.version,\n                     \"isCreatedByUser\": False,\n                 },\n                 include=library_agent_include(user_id),"
        }
      ]
    },
    {
      "sha": "900a661266ff2515d0a8c84ff8980c9a572659f8",
      "message": "fix(backend): Filter out empty object from conversation_history pin on SmartDecisionMakerBlock",
      "changes": [
        {
          "file": "autogpt_platform/backend/backend/blocks/smart_decision_maker.py",
          "patch": "@@ -338,7 +338,7 @@ def run(\n         tool_functions = self._create_function_signature(node_id)\n \n         input_data.conversation_history = input_data.conversation_history or []\n-        prompt = [json.to_dict(p) for p in input_data.conversation_history]\n+        prompt = [json.to_dict(p) for p in input_data.conversation_history if p]\n \n         pending_tool_calls = get_pending_tool_calls(input_data.conversation_history)\n         if pending_tool_calls and not input_data.last_tool_output:"
        }
      ]
    },
    {
      "sha": "55d649515553f937adbcb51e69b408ba9496ebb7",
      "message": "fix(backend): Increase logging level threshold of RPC service to WARNING (#9576)\n\nIncrease the logging threshold to WARNING to avoid chatty RPC service\nrequest logs.\n\nBefore:\n![RPC calls show up in backend\nlog](https://github.com/user-attachments/assets/70791f87-12ef-4a12-8343-4b8641302faa)",
      "changes": [
        {
          "file": "autogpt_platform/backend/backend/util/service.py",
          "patch": "@@ -326,7 +326,12 @@ def __start_fastapi(self):\n             f\"[{self.service_name}] Starting RPC server at http://{api_host}:{self.get_port()}\"\n         )\n         server = uvicorn.Server(\n-            uvicorn.Config(self.fastapi_app, host=api_host, port=self.get_port())\n+            uvicorn.Config(\n+                self.fastapi_app,\n+                host=api_host,\n+                port=self.get_port(),\n+                log_level=\"warning\",\n+            )\n         )\n         self.shared_event_loop.run_until_complete(server.serve())\n "
        }
      ]
    },
    {
      "sha": "d3b83b79f970f3f38de63b562b5cf64ec7e1e78d",
      "message": "fix(backend): Append prompt into the conversations output & Remove unused output pin on SmartDecisionBlock (#9550)\n\n### Changes \ud83c\udfd7\ufe0f\n\nAppend prompt into the conversations output & Remove unused output pin\non SmartDecisionBlock.\n\n### Checklist \ud83d\udccb\n\n#### For code changes:\n- [ ] I have clearly listed my changes in the PR description\n- [ ] I have made a test plan\n- [ ] I have tested my changes according to the test plan:\n  <!-- Put your test plan here: -->\n  - [ ] ...\n\n<details>\n  <summary>Example test plan</summary>\n  \n  - [ ] Create from scratch and execute an agent with at least 3 blocks\n- [ ] Import an agent from file upload, and confirm it executes\ncorrectly\n  - [ ] Upload agent to marketplace\n- [ ] Import an agent from marketplace and confirm it executes correctly\n  - [ ] Edit an agent from monitor, and confirm it executes correctly\n</details>\n\n#### For configuration changes:\n- [ ] `.env.example` is updated or already compatible with my changes\n- [ ] `docker-compose.yml` is updated or already compatible with my\nchanges\n- [ ] I have included a list of my configuration changes in the PR\ndescription (under **Changes**)\n\n<details>\n  <summary>Examples of configuration changes</summary>\n\n  - Changing ports\n  - Adding new services that need to communicate with each other\n  - Secrets or environment variable changes\n  - New or infrastructure changes such as databases\n</details>",
      "changes": [
        {
          "file": "autogpt_platform/backend/backend/blocks/smart_decision_maker.py",
          "patch": "@@ -79,12 +79,7 @@ class Input(BlockSchema):\n         )\n \n     class Output(BlockSchema):\n-\n-        prompt: str = SchemaField(description=\"The prompt sent to the language model.\")\n         error: str = SchemaField(description=\"Error message if the API call failed.\")\n-        function_signatures: list[dict[str, Any]] = SchemaField(\n-            description=\"The function signatures that are sent to the language model.\"\n-        )\n         tools: Any = SchemaField(description=\"The tools that are available to use.\")\n         finished: str = SchemaField(\n             description=\"The finished message to display to the user.\"\n@@ -331,6 +326,9 @@ def run(\n                 for c in response.tool_calls\n             )\n \n+        input_data.conversation_history.append(\n+            llm.Message(role=llm.MessageRole.USER, content=response.prompt)\n+        )\n         input_data.conversation_history.append(\n             llm.Message(role=llm.MessageRole.ASSISTANT, content=assistant_response)\n         )"
        }
      ]
    },
    {
      "sha": "d7cdf751a8751ae9acbe42b21d6164fe7a032cda",
      "message": "fix(backend): allow more than one arg on the tuple (#9535)\n\n<!-- Clearly explain the need for these changes: -->\nWe allow tuples to be returned from exceptions, but pydantic restricts\ntheir size to 1 b/c the typehint. This fixes that\n\n### Changes \ud83c\udfd7\ufe0f\n\n<!-- Concisely describe all of the changes made in this pull request:\n-->\n- Adds `, ...` to the tuple type hint\n\n### Checklist \ud83d\udccb\n\n#### For code changes:\n- [x] I have clearly listed my changes in the PR description\n- [x] I have made a test plan\n- [x] I have tested my changes according to the test plan:\n  <!-- Put your test plan here: -->\n- [x] Extracted from other pr where it was tested as part of an\nexception",
      "changes": [
        {
          "file": "autogpt_platform/backend/backend/util/service.py",
          "patch": "@@ -242,7 +242,7 @@ def cleanup(self):\n \n class RemoteCallError(BaseModel):\n     type: str = \"RemoteCallError\"\n-    args: Optional[Tuple[Any]] = None\n+    args: Optional[Tuple[Any, ...]] = None\n \n \n EXCEPTION_MAPPING = {"
        }
      ]
    },
    {
      "sha": "1011b70d41311f7d34e15acf22493e9cff1950e4",
      "message": "fix(platform): Add Block Costs for SDM Block (#9531)\n\nThe Smart Decision Maker block needs to have costs associated with it.\n\n### Changes \ud83c\udfd7\ufe0f\n\n- Added config for SDM Block Costs",
      "changes": [
        {
          "file": "autogpt_platform/backend/backend/data/block_cost_config.py",
          "patch": "@@ -15,6 +15,7 @@\n     LlmModel,\n )\n from backend.blocks.replicate_flux_advanced import ReplicateFluxAdvancedModelBlock\n+from backend.blocks.smart_decision_maker import SmartDecisionMakerBlock\n from backend.blocks.talking_head import CreateTalkingAvatarVideoBlock\n from backend.blocks.text_to_speech_block import UnrealTextToSpeechBlock\n from backend.data.block import Block\n@@ -265,4 +266,5 @@\n             },\n         )\n     ],\n+    SmartDecisionMakerBlock: LLM_COST,\n }"
        }
      ]
    },
    {
      "sha": "5f5d30a17acf8d49dc8f4e7a43c459a2bba59d00",
      "message": "fix(backend): Use Pyro for RPC by default (#9528)\n\n- Follow-up to #9508 \n\nHTTP-based RPC has not been fully tested and should be disabled by\ndefault.\n\n### Changes \ud83c\udfd7\ufe0f\n\n- Disable HTTP-based RPC and use Pyro by default",
      "changes": [
        {
          "file": "autogpt_platform/backend/backend/util/settings.py",
          "patch": "@@ -66,7 +66,7 @@ class Config(UpdateTrackingModel[\"Config\"], BaseSettings):\n         description=\"Maximum number of workers to use for node execution within a single graph.\",\n     )\n     use_http_based_rpc: bool = Field(\n-        default=True,\n+        default=False,\n         description=\"Whether to use HTTP-based RPC for communication between services.\",\n     )\n     pyro_host: str = Field("
        }
      ]
    }
  ]
}